{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b521c42b",
   "metadata": {},
   "source": [
    "# Modelling \n",
    "\n",
    "In this part I'm going to use data with features prepared earlier to train machine learning model to predict churn.\n",
    "\n",
    "I'm gonna split the full dataset into train and test sets, test out several of the machine learning methods, evaluate the accuracy of the various models and tune parameters as necessary.\n",
    "I will determine winning model based on test accuracy.\n",
    "\n",
    "Since the churned users are a fairly small subset, I will use F1 score as the metric to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d7d0564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "#import numpy\n",
    "#from numpy import allclose\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression, GBTClassifier, LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "991a6354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Sparkify_Project\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ee14cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading cleaned dataset\n",
    "path = \"mini_sparkify_data_features.json\"\n",
    "data_features = spark.read.json(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff45a7e",
   "metadata": {},
   "source": [
    "Firsty let's tranform our dataset by adding vector of features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "434db967",
   "metadata": {},
   "outputs": [],
   "source": [
    "numericCols = ['number_of_songs_played', 'number_of_thumbs_down', 'number_of_thumbs_up', 'number_of_roll_advert', 'number_of_add_to_playlist', 'number_of_add_friend', 'number_of_errors', 'time_on_paid_version', 'time_whole']\n",
    "assembler = VectorAssembler(inputCols=numericCols, outputCol=\"features\")\n",
    "df_assembled = assembler.transform(data_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4146ac84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------------------------------------------------+\n",
      "|userId|features                                                  |\n",
      "+------+----------------------------------------------------------+\n",
      "|100010|[275.0,5.0,17.0,52.0,7.0,4.0,0.0,0.0,3820418.0]           |\n",
      "|200002|[387.0,6.0,21.0,7.0,8.0,4.0,0.0,2469195.0,3930924.0]      |\n",
      "|125   |(9,[0,3,8],[8.0,1.0,1774.0])                              |\n",
      "|124   |[4079.0,41.0,171.0,4.0,118.0,74.0,6.0,5183736.0,5183736.0]|\n",
      "|51    |[2111.0,21.0,100.0,0.0,52.0,28.0,1.0,1363340.0,1363340.0] |\n",
      "|7     |[150.0,1.0,7.0,16.0,5.0,1.0,1.0,0.0,4387742.0]            |\n",
      "|15    |[1914.0,14.0,81.0,1.0,59.0,31.0,2.0,4732403.0,4732403.0]  |\n",
      "|54    |[2841.0,29.0,163.0,47.0,72.0,33.0,1.0,3697678.0,3697678.0]|\n",
      "|155   |[820.0,3.0,58.0,8.0,24.0,11.0,3.0,1676949.0,2231525.0]    |\n",
      "|100014|[257.0,3.0,17.0,2.0,7.0,6.0,0.0,3563513.0,3563513.0]      |\n",
      "+------+----------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_assembled.select(\"userId\", \"features\").show(10,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbb2a87",
   "metadata": {},
   "source": [
    "Also let's transform label column to numerical:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8519f3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_stringIdx = StringIndexer(inputCol = 'label', outputCol = 'labelIndex')\n",
    "df_final = label_stringIdx.fit(df_assembled).transform(df_assembled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "579fcf70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+----------+\n",
      "|userId|label|labelIndex|\n",
      "+------+-----+----------+\n",
      "|100010|    0|       0.0|\n",
      "|200002|    0|       0.0|\n",
      "|   125|    1|       1.0|\n",
      "|   124|    0|       0.0|\n",
      "|    51|    1|       1.0|\n",
      "+------+-----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_final.select(\"userId\", \"label\", \"labelIndex\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934cb35d",
   "metadata": {},
   "source": [
    "Now we can split our dataset into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3648a02c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 163\n",
      "Test Dataset Count: 62\n"
     ]
    }
   ],
   "source": [
    "train, test = df_final.randomSplit([0.7, 0.3], seed=42)\n",
    "print(\"Training Dataset Count: \" + str(train.count()))\n",
    "print(\"Test Dataset Count: \" + str(test.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7795c4",
   "metadata": {},
   "source": [
    "Also let's create function for printing metrics of our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3cdd5099",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(predictions):\n",
    "    true_positives = predictions.select(\"labelIndex\", \"prediction\").filter((F.col(\"labelIndex\") == 1.0) & (F.col(\"prediction\") == 1.0)).count()\n",
    "    true_negatives = predictions.select(\"labelIndex\", \"prediction\").filter((F.col(\"labelIndex\") == 0.0) & (F.col(\"prediction\") == 0.0)).count()\n",
    "    false_positives = predictions.select(\"labelIndex\", \"prediction\").filter((F.col(\"labelIndex\") == 0.0) & (F.col(\"prediction\") == 1.0)).count()\n",
    "    false_negatives = predictions.select(\"labelIndex\", \"prediction\").filter((F.col(\"labelIndex\") == 1.0) & (F.col(\"prediction\") == 0.0)).count()\n",
    "    \n",
    "    accuracy = (true_positives + true_negatives) / (true_positives + true_negatives + false_positives + false_negatives)\n",
    "    print(\"Accuracy: \", accuracy)\n",
    "    precision = true_positives / (true_positives + false_positives)\n",
    "    print(\"Precision: \", precision)\n",
    "    recall = true_positives / (true_positives + false_negatives)\n",
    "    print(\"Recall: \", recall)\n",
    "    #F1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    F1_score = (2 * true_positives) / (2 * true_positives + false_positives + false_negatives)\n",
    "    print(\"F1 score: \", F1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdcf0fa",
   "metadata": {},
   "source": [
    "Now we are ready to train our models, let's start with \n",
    "### Random Forest Classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35801a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(featuresCol = 'features', labelCol = 'labelIndex')\n",
    "rfModel = rf.fit(train)\n",
    "predictions_rf = rfModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "961403e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8709677419354839\n",
      "Precision:  0.5833333333333334\n",
      "Recall:  0.7\n",
      "F1 score:  0.6363636363636364\n"
     ]
    }
   ],
   "source": [
    "print_metrics(predictions_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6793f03e",
   "metadata": {},
   "source": [
    "### Logistics Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ece2fa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(featuresCol = 'features', labelCol = 'labelIndex')\n",
    "lrModel = lr.fit(train)\n",
    "predictions_lr = lrModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a8694b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8709677419354839\n",
      "Precision:  0.6\n",
      "Recall:  0.6\n",
      "F1 score:  0.6\n"
     ]
    }
   ],
   "source": [
    "print_metrics(predictions_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab2851d",
   "metadata": {},
   "source": [
    "### SVC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3bbdefd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = LinearSVC(featuresCol = 'features', labelCol = 'labelIndex')\n",
    "svcModel = svc.fit(train)\n",
    "predictions_svc = svcModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b033f43b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8387096774193549\n",
      "Precision:  0.5\n",
      "Recall:  0.7\n",
      "F1 score:  0.5833333333333334\n"
     ]
    }
   ],
   "source": [
    "print_metrics(predictions_svc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9895f021",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "72c4f67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt = GBTClassifier(featuresCol = 'features', labelCol = 'labelIndex')\n",
    "gbtModel = gbt.fit(train)\n",
    "predictions_gbt = gbtModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2a5afc34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8225806451612904\n",
      "Precision:  0.4666666666666667\n",
      "Recall:  0.7\n",
      "F1 score:  0.56\n"
     ]
    }
   ],
   "source": [
    "print_metrics(predictions_gbt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e56af6",
   "metadata": {},
   "source": [
    "As we can see the best model is Logistics Regression with accuracy of 87%. the most important metrics in this case is precision as we want to predict as many true positives as posibble. For our Logistic regression model it's 0.6, which is not a lot but if we tak einto account that we work on a very small dataset everything more that 50% is a good result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc649a13",
   "metadata": {},
   "source": [
    "### Model tuning\n",
    "\n",
    "Now let's try to tune our model by applying grid search \n",
    "\n",
    "This code is based on an example from this website:\n",
    "https://spark.apache.org/docs/latest/ml-tuning.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "98a82cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "pipeline = Pipeline(stages=[lr])\n",
    "\n",
    "# We now treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n",
    "# This will allow us to jointly choose parameters for all Pipeline stages.\n",
    "# A CrossValidator requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n",
    "# We use a ParamGridBuilder to construct a grid of parameters to search over.\n",
    "# With 3 values for lr.maxIter, 3 values for lr.aggregationDepth and 2 values for lr.fitIntercept\n",
    "# this grid will have 3 x 3 x 2 = 18 parameter settings for CrossValidator to choose from.\n",
    "\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.maxIter, [100, 200, 300]) \\\n",
    "    .addGrid(lr.aggregationDepth, [2, 3, 4]) \\\n",
    "    .addGrid(lr.fitIntercept, [True, False]) \\\n",
    "    .build()\n",
    "\n",
    "# We use f1 score as a metric for optimization\n",
    "mce = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"labelIndex\", metricName='fMeasureByLabel', metricLabel=1, beta=1.0)\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=mce,\n",
    "                          numFolds=2) \n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cvModel = crossval.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fb8649dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_cvModel = cvModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4b8255c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8709677419354839\n",
      "Precision:  0.6\n",
      "Recall:  0.6\n",
      "F1 score:  0.6\n"
     ]
    }
   ],
   "source": [
    "print_metrics(prediction_cvModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ece11ee",
   "metadata": {},
   "source": [
    "As we can see parameters tuning did not improve our model, but this might still work when using bigger dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4b12bf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2326aad6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
